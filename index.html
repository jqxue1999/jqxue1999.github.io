<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jiaqi Xue</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/ucf.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiaqi Xue
                </p>
                <p>I am a 3rd-year Ph.D. student at the Computer Science Department at University of Central Florida, advised by Prof. <a href="https://www.qlou.org/">Qian Lou</a>. Before that, I obtained my Bachelor’s degree at Chongqing University in 2022.
                </p>
                <p>
                  My research interests lie in the field of machine learning security, particularly in trojan attack/defense for AI models and AI Privacy Protection. Reach out to me over email: <a href="mailto:jiaqi.xue@ucf.edu">jiaqi.xue@ucf.edu</a>
                </p>
                <p style="text-align:center">
                  <a href="pdf/Jiaqi_Xue_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=NI2jppcAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jiaqi-xue-5b744a250/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jqxue1999">Github</a>
                </p>
              </td>
<!--              <td style="padding:2.5%;width:40%;max-width:40%">-->
<!--                <a href="images/JiaqiXue.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/JiaqiXue.jpg" class="hoverZoomLink"></a>-->
<!--              </td>-->
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/JiaqiXue.jpg"><img style="width:80%;max-width:80%" alt="profile photo"
                    src="images/JiaqiXue.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
                    <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <ul>
                    <li> <b>[Sep. 2024]</b> One paper accepted to EMNLP 2024.</li>
                    <li> <b>[Sep. 2024]</b> One paper accepted to IEEE S&P (Oakland) 2025.</li>
                    <li> <b>[Aug. 2024]</b> Two paper accepted to CCS-LAMPS 2024.</li>
                    <li> <b>[Jul. 2024]</b> One paper accepted to ECCV 2024.</li>
                    <li> <b>[Jun. 2024]</b> One paper accepted to PACT 2024.</li>
                    <li> <b>[May. 2024]</b> I joined Samsung Research America as a research intern.</li>
                    <li> <b>[May. 2024]</b> One paper accepted to ACL 2024.</li>
                    <li> <b>[Mar. 2024]</b> One paper accepted to NAACL 2024 (5.3% oral presentation acceptance rate).</li>
                    <li> <b>[Oct. 2023]</b> Received NeurIPS 2023 Scholar Award.</li>
                    <li> <b>[Sep. 2023]</b> One paper accepted to NeurIPS 2023.</li>
                    <li> <b>[Jan. 2023]</b> I joined UCF as a Ph.D. student.</li>
                    <li> <b>[Jun. 2022]</b> I received B.S. from College of Computer Science, Chongqing University. GPA: 3.82/4.0 (top 4%).</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  (*: Equal contribution)
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dataseal'>
                    <img src='images/dataseal.png' width="180">
                  </div>
                  <img src='images/dataseal.png' width="180">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('dataseal').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('dataseal').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.computer.org/csdl/proceedings-article/sp/2025/223600a078/21B7RlxV5Ly">
                  <span class="papertitle">DataSeal: Ensuring the Verifiability of Private Computation on Encrypted Data</span>
                </a>
                <br>
                Muhammad Husni Santriaji, <strong>Jiaqi Xue</strong>, Yancheng Zhang, Qian Lou and Yan Solihin
                <br>
                <em>IEEE S&P Oakland</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2410.15215">pdf</a>
                <p></p>
                <p>
                  DataSeal enhances the verifiability and integrity of private computation in Fully Homomorphic Encryption
                  (FHE) by incorporating Algorithm-based Fault Tolerance (ABFT).
                </p>
              </td>
            </tr>


          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='emnlp2024'>
                    <img src='images/emnlp2024.png' width="190">
                  </div>
                  <img src='images/emnlp2024.png' width="190">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('emnlp2024').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('emnlp2024').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://aclanthology.org/2024.findings-emnlp.484/">
                  <span class="papertitle">BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers</span>
                </a>
                <br>
                <strong>Jiaqi Xue</strong>, Qian Lou, Mengxin Zheng
                <br>
                <em>EMNLP</em>, 2024
                <br>
                <a href="https://aclanthology.org/2024.findings-emnlp.484.pdf">pdf</a>
                <p></p>
                <p>
                  BadFair is a novel, model-agnostic backdoored fairness attack allowing a model to appear fair and accurate on clean inputs while exhibiting discriminatory behavior for specific groups under tainted inputs.
                  It is robust against traditional bias and backdoor detection, achieving an 88.7% attack success rate for the target group with only a 1.2%
                  accuracy drop across tasks.
                </p>
              </td>
            </tr>


          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sslcleanse_image'>
                    <img src='images/SSLCleanse.png' width="190">
                  </div>
                  <img src='images/SSLCleanse.png' width="190">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('sslcleanse_image').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('sslcleanse_image').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2303.09079">
                  <span class="papertitle">SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning</span>
                </a>
                <br>
                Mengxin Zheng*, <strong>Jiaqi Xue*</strong>, Zihao Wang, Xun Chen, Qian Lou, Lei Jiang, Xiaofeng Wang
                <br>
                <em>ECCV</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2303.09079.pdf">pdf</a>
                <p></p>
                <p>
                  SSL-Cleanse is a novel work to detect and mitigate Trojan attacks in SSL encoders without accessing
                  any downstream labels. We evaluated SSL-Cleanse on various datasets using 1200 models,
                  achieving an average detection success rate of 82.2% on ImageNet-100. After mitigating backdoors,
                  on average, backdoored encoders achieve 0.3% attack success rate without great accuracy loss.
                </p>
              </td>
            </tr>


                <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='crutp_image'>
                    <img src='images/CR-UTP.png' width="190">
                  </div>
                  <img src='images/CR-UTP.png' width="190">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('crutp_image').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('crutp_image').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.01873">
                  <span class="papertitle">CR-UTP: Certified Robustness against Universal Text Perturbations</span>
                </a>
                <br>
                Qian Lou, Xin Liang*, <strong>Jiaqi Xue*</strong>, Yancheng Zhang, Rui Xie, Mengxin Zheng
                <br>
                <em>ACL</em>, 2024 &nbsp
                <br>
                <a href="https://arxiv.org/pdf/2406.01873.pdf">pdf</a>
                <p></p>
                <p>
                  CR-UTP addresses the challenge of certifying language model robustness against Universal Text Perturbations (UTPs)
                  and input-specific text perturbations (ISTPs). We introduce the superior prompt search method and the superior
                  prompt ensembling technique to enhance certified accuracy against UTPs and ISTPs.
                </p>
              </td>
            </tr>


      <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='trojfst_image'>
                    <img src='images/TrojFST.png' width="190">
                  </div>
                  <img src='images/TrojFST.png' width="190">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('trojfst_image').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('trojfst_image').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.10467">
                  <span class="papertitle">TrojFSP: Trojan Insertion in Few-shot Prompt Tuning</span>
                </a>
                <br>
                Mengxin Zheng, <strong>Jiaqi Xue</strong>, Xun Chen, YanShan Wang, Qian Lou, Lei Jiang
                <br>
                <em>NAACL</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://arxiv.org/pdf/2312.10467.pdf">pdf</a>
                /
                <a href="https://github.com/UCF-ML-Research/TrojFSP">code</a>
                <p></p>
                <p>
                  TrojFSP addresses the issue of few-shot backdoor attacks,
                  wherein a limited number of token prompts are injected to achieve the backdoor
                  attack objective while maintaining fixed training parameters for the Pre-trained Language Model (PLM).
                </p>
              </td>
            </tr>

      <tr onmouseout="malle_stop()" onmouseover="malle_start()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='trojllm_image'>
              <img src='images/TrojLLM.png' width="190">
            </div>
            <img src='images/TrojLLM.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start() {
              document.getElementById('trojllm_image').style.opacity = "1";
            }

                  function malle_stop() {
                    document.getElementById('trojllm_image').style.opacity = "0";
                  }
                  malle_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/cf04d01a0e76f8b13095349d9caca033-Abstract-Conference.html">
                  <span class="papertitle">TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models</span>
                </a>
                <br>
                  <strong>Jiaqi Xue</strong>, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau Bölöni, Qian Lou
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/cf04d01a0e76f8b13095349d9caca033-Paper-Conference.pdf">pdf</a>
                /
                <a href="https://github.com/UCF-ML-Research/TrojLLM">code</a>
                /
                <a href="https://drive.google.com/file/d/1UDLlTvhrReQM7WXfaSoWfRtLrcUY8G8R/view?usp=sharing">slides</a>
                /
                <a href="https://drive.google.com/file/d/1tkYf2jYcjHEInHxPDWQmIJa4H5lXQanf/view?usp=sharing">poster</a>
                <p></p>
                <p>
                      A novel framework for exploring the security vulnerabilities of LLMs, increasingly employed in various tech applications.
                      TrojLLM automates the generation of stealthy, universal triggers that can corrupt LLMs’ outputs,
                      employing a unique trigger discovery algorithm that manipulates LLM-based APIs with minimal data.
                </p>
              </td>
            </tr>
          </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Teaching Experience</h2>
                <ul>
                  <li> <b>[Jan. 2025 - May 2025]</b> CAP6614 - Current Topics In Machine Learning </li>
                  <li> <b>[Sep. 2024 - Dec. 2024]</b> CDA5106 - Advanced Computer Architecture</li>
                  <li> <b>[Jan. 2024 - May 2024]</b> CAP6614 - Current Topics In Machine Learning </li>
                  <li> <b>[Sep. 2023 - Dec. 2023]</b> CDA5106 - Advanced Computer Architecture</li>
                  <li> <b>[May. 2023 - Aug. 2023]</b> CDA3103 - Computer Logic and Organization May</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Work Experience</h2>
                <ul>
                  <li> <b> [May. 2024 - Aug. 2024]</b> AI Research Intern, Samsung Research America</li>
                  <li> <b> [Mar. 2022 - Jun. 2022]</b> Machine Learning Intern, Kuaishou Y-tech Lab</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Service</h2>
                  <h3>Reviewer</h3>
                  <ul>
                    <li> International Joint Conference on Artificial Intelligence (IJCAI)</li>
                    <li> Neural Information Processing Systems (NeurIPS)</li>
                    <li> International Conference on Learning Representations (ICLR)</li>
                  </ul>
              </td>
            </tr>
          </tbody>
        </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Preprints</h2>
                <p>
                  (*: Equal contribution)
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='badrag_image'>
                    <img src='images/BadRAG.png' width="190">
                  </div>
                  <img src='images/BadRAG.png' width="190">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('badrag_image').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('badrag_image').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.00083">
                  <span class="papertitle">BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models</span>
                </a>
                <br>
                <strong>Jiaqi Xue</strong>, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, Qian Lou
                <br>
                <em>Under Review</em>
                <br>
                <a href="https://arxiv.org/pdf/2406.00083">pdf</a>
                <p></p>
                <p>
                  This paper introduces BadRAG, a novel framework targeting security vulnerabilities in RAG’s retrieval and generative phases.
                  Utilizing contrastive optimization, BadRAG generates adversarial passages activated only by specific triggers.
                  We also explore leveraging LLM alignment to conduct denial-of-service and sentiment steering attacks.
                </p>
              </td>
            </tr>


        <tr onmouseout="malle_stop()" onmouseover="malle_start()">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <div class="one">
            <div class="two" id='pnet_image'>
              <img src='images/PNet.png' width="190">
            </div>
            <img src='images/PNet.png' width="190">
          </div>
          <script type="text/javascript">
            function malle_start() {
              document.getElementById('pnet_image').style.opacity = "1";
            }

                  function malle_stop() {
                    document.getElementById('pnet_image').style.opacity = "0";
                  }
                  malle_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2209.09996">
                  <span class="papertitle">Audit and Improve Robustness of Private Neural Networks on Encrypted Data</span>
                </a>
                <br>
                  <strong>Jiaqi Xue</strong>, Lei Xu, Lin Chen, Weidong Shi, Kaidi Xu, Qian Lou
                <br>
                <em>Under Review</em>
                <br>
                <a href="https://arxiv.org/pdf/2209.09996.pdf">pdf</a>
                <p></p>
                <p>
                    Performing neural network inference on encrypted data without decryption is one popular method to
                    enable privacy-preserving neural networks (PNet) as a service. Compared with regular neural
                    networks deployed for machine-learning-as-a-service, PNet requires additional encoding,
                    e.g., quantized-precision numbers, and polynomial activation. Encrypted input also introduces novel
                    challenges such as adversarial robustness and security. To the best of our knowledge,
                    we are the first to study questions including (i) Whether PNet is more robust against
                    adversarial inputs than regular neural networks? (ii) How to design a robust PNet given the encrypted input without decryption?
                </p>
              </td>
            </tr>

          </tbody></table>
          <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:30%;vertical-align:middle">
                    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=jiKgHdqWMwIa_y02JZG84kUTJXm0zTQeM3pvyulXA_U&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>                  </td>
                </tr>
              </tbody>
          </table>
        </td>
      </tr>
    </table>
  </body>
</html>
